{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoders\n",
    "\n",
    "In this exercise, I will try to build a Variational Auto-Encoder using PyTorch library. Next, VAE will be trained on MNIST dataset with a subsequent analysis of the results.\n",
    "\n",
    "Special thanks to **Datacamp** website for their crash course on Pytorch.\n",
    "Also to **Yandex Data Science School**, their Deep Learning course was helpful.\n",
    "Finally, thanks to the **PyTorch development team** for this example: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "\n",
    "*Disclaimer*: VAE class implemented here is an almost identical copy of the VAE class that can be found by the link above. The main goal for me was to expand my knowledge of PyTorch, analyze an example of good usage of PyTorch and understand VAEs, and NOT to create an authentic piece of software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make necessary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.optim import *\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std of the MNIST dataset (can be googled)\n",
    "MNIST_mean = 0.1307\n",
    "MNIST_std  = 0.3081\n",
    "\n",
    "# batch size for an epoch of training, changeable\n",
    "MNIST_batch_size = 32\n",
    "\n",
    "MNIST_transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "# prepare train and test sets\n",
    "trainset = torchvision.datasets.MNIST(\"mnist\", train=True, download=True, transform=MNIST_transform)\n",
    "testset  = torchvision.datasets.MNIST(\"mnist\", train=False, download=True, transform=MNIST_transform)\n",
    "\n",
    "# prepare data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = MNIST_batch_size, shuffle=True)\n",
    "testloader  = torch.utils.data.DataLoader(testset, batch_size=MNIST_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating VAE net\n",
    "In this section we will try and implement VAE using PyTorch. This section may be subject to future updates and experiments in order to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1  = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3  = nn.Linear(20, 400)\n",
    "        self.fc4  = nn.Linear(400, 784)\n",
    "    \n",
    "    \n",
    "    def encode(self, X):\n",
    "        \"\"\"\n",
    "        encodes the inout image into two vectors: mean and variance\n",
    "        :param X: input image in torch Tensor format\n",
    "        :returns: mu and var\n",
    "        \"\"\"\n",
    "        hidden1 = F.relu(self.fc1(X))\n",
    "        return self.fc21(hidden1), self.fc22(hidden1)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        implementation of the reparameterization trick, allowing for training with random sampling\n",
    "        :param mu: mean values tensor\n",
    "        :param var: logvariance tensor\n",
    "        :returns: random tensor from the Gaussian distribution\n",
    "        \"\"\"\n",
    "        # get variance\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        \n",
    "        # get random tensor from normal distribution of mean 0 and var 1 of size like std\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu+eps*std\n",
    "    \n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        project a tensor from the latent space back into original coordinates\n",
    "        :param z: tensor in the latent space to be decoded\n",
    "        \"\"\"\n",
    "        hidden3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(hidden3))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward function of VAE NN\n",
    "        :param x: input image in torch Tensor format\n",
    "        returns: x decoded from latent space along with mean and logvar tensors\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a model\n",
    "model = VAE()\n",
    "\n",
    "# instantiate an Adam optimizer with L2-regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 0.001)\n",
    "\n",
    "# define custom loss-function\n",
    "def loss_func(reconstructed_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    defines a loss for our VAE. Consists of\n",
    "    a) Reconstruction loss\n",
    "    b) Kullback-Leibler divergence losses summed over all elements and the batch\n",
    "    \"\"\"\n",
    "\n",
    "    reconstr_loss = F.binary_cross_entropy(reconstructed_x, x.view(-1, 784), reduction='sum')\n",
    "    KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstr_loss+KLd\n",
    "\n",
    "\n",
    "def train_VAE(epoch):\n",
    "    # put model in train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(trainloader):\n",
    "        # zero all gradients\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_batch, mu, logvar = model(data)\n",
    "        loss = loss_func(reconstructed_batch, data, mu, logvar)\n",
    "        \n",
    "        # do backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(trainloader.dataset)))\n",
    "    \n",
    "def test_VAE(epoch):\n",
    "    # put model into evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    # deactivate autograd engine (backprop become unavailable, but that speeds up computations and\n",
    "    # reduces memory usage; also, we don't update weights here, so backprop is not needed).\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(testloader):\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_func(recon_batch, data, mu, logvar).item()\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying VAE in MNIST dataset\n",
    "In this section we will experiment with VAE and use everything we prepared up until this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 135.7474\n",
      "====> Test set loss: 115.0611\n",
      "====> Epoch: 2 Average loss: 113.1201\n",
      "====> Test set loss: 110.2719\n",
      "====> Epoch: 3 Average loss: 110.0302\n",
      "====> Test set loss: 108.1107\n",
      "====> Epoch: 4 Average loss: 108.5750\n",
      "====> Test set loss: 107.5699\n",
      "====> Epoch: 5 Average loss: 107.7129\n",
      "====> Test set loss: 107.1990\n",
      "====> Epoch: 6 Average loss: 107.1025\n",
      "====> Test set loss: 106.3990\n",
      "====> Epoch: 7 Average loss: 106.5550\n",
      "====> Test set loss: 106.3621\n",
      "====> Epoch: 8 Average loss: 106.1665\n",
      "====> Test set loss: 105.8615\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 10\n",
    "for epoch in range(1, EPOCH_NUM + 1):\n",
    "    train_VAE(epoch)\n",
    "    test_VAE(epoch)\n",
    "    with torch.no_grad():\n",
    "        # let's try to decode a random vector from our latent space!\n",
    "        sample = torch.randn(64, 20)\n",
    "        sample = model.decode(sample)\n",
    "        save_image(sample.view(64, 1, 28, 28),'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
